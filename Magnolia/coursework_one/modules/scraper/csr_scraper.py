import os
import sys
import time
import datetime
import urllib.parse
from concurrent.futures import ThreadPoolExecutor

import requests
from apscheduler.schedulers.blocking import BlockingScheduler

# Selenium / ChromeDriver
import chromedriver_autoinstaller
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# PostgreSQL åªç”¨äºè¯»å–å…¬å¸åˆ—è¡¨
import psycopg2

# MongoDB (æ”¹ä¸ºå†™å…¥ï¼‰
from pymongo import MongoClient

# MinIO
from minio import Minio

# ========== é…ç½®åŒº ==========
DB_CONFIG = {
    "dbname": "fift",
    "user": "postgres",
    "password": "postgres",
    "host": "localhost",
    "port": "5439",
}

MONGO_URI = "mongodb://localhost:27019"
MONGO_DB_NAME = "csr_db"
MONGO_COLLECTION = "csr_reports"

mongo_client = MongoClient(MONGO_URI)
mongo_db = mongo_client[MONGO_DB_NAME]
collection_reports = mongo_db[MONGO_COLLECTION]

MINIO_CLIENT = Minio(
    "localhost:9000",
    access_key="ift_bigdata",
    secret_key="minio_password",
    secure=False,
)
BUCKET_NAME = "csr-reports"

PROXY = None  # å¦‚æœéœ€è¦ä»£ç†ï¼Œå¦‚ï¼š"http://127.0.0.1:7890"

# ========== æ—¥å¿—åŠŸèƒ½ ==========
# 1) åœ¨æµ‹è¯•ç¯å¢ƒä¸‹ï¼Œæ—¥å¿—å†™åˆ° "test_log.log"
#    å¦åˆ™æ­£å¸¸å†™å…¥ "csr_fast.log"
if "pytest" in sys.modules:
    LOG_FILE = "test_log.log"
else:
    LOG_FILE = "csr_fast.log"

def write_log(message: str):
    """
    è®°å½•æ—¥å¿—åˆ°æ–‡ä»¶å’Œç»ˆç«¯
    å¦‚æœå¤„äº pytest ç¯å¢ƒï¼Œåˆ™å†™å…¥ test_log.log
    å¦åˆ™å†™å…¥ csr_fast.log
    """
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_msg = f"[{timestamp}] {message}"

    # æ‰“å°åˆ°ç»ˆç«¯
    print(log_msg)

    # å†™å…¥æ—¥å¿—æ–‡ä»¶
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(log_msg + "\n")


# ========== æ ¸å¿ƒåŠŸèƒ½ ==========
def init_driver():
    """åˆå§‹åŒ– Chrome WebDriver"""
    write_log("ğŸš€ åˆå§‹åŒ– ChromeDriver...")

    options = webdriver.ChromeOptions()
    # å¦‚æœä¸éœ€è¦æ‰“å¼€æµè§ˆå™¨ç•Œé¢ï¼Œå¯å–æ¶ˆæ³¨é‡Šä»¥æ— å¤´æ¨¡å¼
    # options.add_argument('--headless')
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--log-level=3")

    # è‹¥éœ€è¦ä»£ç†
    # if PROXY:
    #     options.add_argument(f'--proxy-server={PROXY}')

    chromedriver_autoinstaller.install()
    driver = webdriver.Chrome(options=options)

    write_log("âœ… ChromeDriver å¯åŠ¨æˆåŠŸï¼")
    return driver


def get_search_results(driver, query, timeout=5):
    """
    åœ¨ Bing ä¸Šæœç´¢, è¿”å›æœç´¢ç»“æœ
    1) ç¼©çŸ­é»˜è®¤è¶…æ—¶åˆ° 5s
    2) è‹¥ driver è¢« mockï¼Œåˆ™ç›´æ¥è¿”å› mock è®¾å®šçš„ç»“æœ
    """
    # å¦‚æœ driver æ˜¯ mockï¼ˆpytest å¯¹ get_search_results è¿›è¡Œ patchï¼‰ï¼Œ
    # å¯èƒ½è¿”å› MagicMock è€Œä¸ä¼šæ‰§è¡Œå®é™…çš„æŸ¥æ‰¾é€»è¾‘ã€‚
    from unittest.mock import MagicMock
    if isinstance(driver, MagicMock):
        return driver.find_elements()

    search_url = f"https://www.bing.com/search?q={urllib.parse.quote(query)}"
    write_log(f"ğŸ” è®¿é—®æœç´¢å¼•æ“: {search_url}")

    try:
        driver.get(search_url)
        WebDriverWait(driver, timeout).until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".b_algo h2 a"))
        )
        results = driver.find_elements(By.CSS_SELECTOR, ".b_algo h2 a")
        write_log(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        return results
    except Exception as e:
        write_log(f"âŒ æœç´¢å¤±è´¥: {type(e).__name__}, {e}")
        return []


def download_pdf(company_name, year, url):
    """ä¸‹è½½ PDF åˆ°æœ¬åœ°(å«å¹´ä»½åŒºåˆ†)"""
    write_log(f"ğŸ“¥ å¼€å§‹ä¸‹è½½ {company_name}({year}) çš„ PDF: {url}")

    if "pdf" not in url.lower():
        write_log(f"âš ï¸ {company_name}({year}) ä¸æ˜¯ PDF æ–‡ä»¶ï¼Œè·³è¿‡")
        return None

    local_dir = "./reports"
    os.makedirs(local_dir, exist_ok=True)
    # é¿å…è¦†ç›–ï¼šåœ¨æœ¬åœ°åŠ å¹´ä»½
    local_path = os.path.join(local_dir, f"{company_name}_{year}.pdf")

    try:
        resp = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
        if resp.status_code == 200:
            with open(local_path, "wb") as f:
                f.write(resp.content)
            write_log(f"âœ… {company_name}({year}) ä¸‹è½½æˆåŠŸ: {local_path}")
            return local_path
        else:
            write_log(f"âŒ {company_name}({year}) ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {resp.status_code}")
    except Exception as e:
        write_log(f"âŒ {company_name}({year}) ä¸‹è½½å¤±è´¥: {type(e).__name__}, {e}")

    return None


def upload_to_minio(company_name, year, local_path):
    """ä¸Šä¼  PDF åˆ° MinIOï¼ŒæŒ‰å¹´ä»½åŒºåˆ†æ–‡ä»¶"""
    try:
        object_name = f"{year}/{company_name}.pdf"
        write_log(f"ğŸ“¤ å¼€å§‹ä¸Šä¼  {company_name}({year}) åˆ° MinIO...")

        with open(local_path, "rb") as f:
            MINIO_CLIENT.put_object(
                bucket_name=BUCKET_NAME,
                object_name=object_name,
                data=f,
                length=os.path.getsize(local_path),
                content_type="application/pdf",
            )
        write_log(f"âœ… MinIO ä¸Šä¼ æˆåŠŸ: {object_name}")
        return object_name
    except Exception as e:
        write_log(f"âŒ MinIO ä¸Šä¼ å¤±è´¥: {type(e).__name__}, {e}")
        return None


def save_csr_report_info_to_mongo(company_name, pdf_url, object_name, year):
    """ä¿å­˜ CSR æŠ¥å‘Šä¿¡æ¯åˆ° MongoDBï¼Œå¹¶è®°å½•æŠ¥å‘Šå¹´ä»½"""
    try:
        data = {
            "company_name": company_name,
            "csr_report_url": pdf_url,
            "storage_path": object_name,
            "csr_report_year": year,
            # å»ºè®®ä½¿ç”¨å¸¦æ—¶åŒºçš„ nowï¼Œå¦‚ datetime.datetime.now(datetime.UTC)
            "ingestion_time": datetime.datetime.utcnow(),
        }
        # ç¡®ä¿ (company + year) åšåŒºåˆ†
        mongo_db["csr_reports"].update_one(
            {"company_name": company_name, "csr_report_year": year},
            {"$set": data},
            upsert=True,
        )
        write_log(f"âœ… MongoDB è®°å½•æ›´æ–°æˆåŠŸ: {company_name}({year})")
    except Exception as e:
        write_log(f"âŒ MongoDB è®°å½•æ›´æ–°å¤±è´¥: {type(e).__name__}, {e}")


def get_company_list_from_postgres():
    """ä» PostgreSQL è·å–å…¬å¸åˆ—è¡¨"""
    write_log("ğŸ” è¿æ¥ PostgreSQL è¯»å–å…¬å¸åˆ—è¡¨...")
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        cur.execute("SELECT security FROM csr_reporting.company_static;")
        rows = cur.fetchall()
        cur.close()
        conn.close()
        companies = [row[0] for row in rows]
        write_log(f"âœ… è·å–åˆ° {len(companies)} å®¶å…¬å¸")
        return companies
    except Exception as e:
        write_log(f"âŒ PostgreSQL è¯»å–å¤±è´¥: {type(e).__name__}, {e}")
        return []


def search_by_years(driver, company_name, years, keywords):
    """
    å¾ªç¯(å¹´ä»½ + å…³é”®è¯)æœç´¢
    å¦‚æœæ‰¾åˆ°PDFåˆ™ä¸‹è½½å¹¶ä¿å­˜, å¹¶è¡Œæœç´¢é€Ÿåº¦æ›´å¿«
    """
    found_any = False
    for year in years:
        # æ–­ç‚¹ç»­çˆ¬: å¦‚æœ MongoDB å·²æœ‰è¯¥å¹´ä»½è®°å½•ï¼Œåˆ™è·³è¿‡
        existing = mongo_db["csr_reports"].find_one(
            {"company_name": company_name, "csr_report_year": year}
        )
        if existing:
            write_log(f"âš ï¸ {company_name}({year}) å·²å­˜åœ¨äºMongoDB, è·³è¿‡")
            continue

        for kw in keywords:
            query = f"{company_name} {year} {kw}"
            write_log(f"ğŸš€ æœç´¢å…³é”®è¯: {query}")
            results = get_search_results(driver, query, timeout=5)

            for r in results:
                url = r.get_attribute("href")
                pdf_path = download_pdf(company_name, year, url)
                if pdf_path:
                    obj_name = upload_to_minio(company_name, year, pdf_path)
                    if obj_name:
                        save_csr_report_info_to_mongo(company_name, url, obj_name, year)
                    found_any = True
                    if os.path.exists(pdf_path):
                        os.remove(pdf_path)
            # å°† sleep ç¼©çŸ­åˆ° 0.2
            time.sleep(0.2)

    return found_any


def search_and_process(company_name):
    """æœç´¢ã€ä¸‹è½½ã€ä¸Šä¼ (ä¸åŒå¹´ä»½)çš„CSRæŠ¥å‘Š"""
    write_log(f"ğŸš€ å¼€å§‹å¤„ç†å…¬å¸: {company_name}")

    driver = None
    try:
        driver = init_driver()

        # éœ€è¦æœç´¢çš„å¹´ä»½ (2020~2024)
        years = range(2020, 2025)
        # å‡å°‘å…³é”®è¯æ•°é‡ï¼ŒåŠ é€Ÿæœç´¢
        keywords = [
            "corporate sustainability report filetype:pdf",
            "ESG report filetype:pdf",
        ]
        found = search_by_years(driver, company_name, years, keywords)
        if not found:
            write_log(f"âš ï¸ {company_name} æœªæ‰¾åˆ°ä»»ä½•PDF")
    except Exception as e:
        write_log(f"âŒ å¤„ç† {company_name} å¤±è´¥: {type(e).__name__}, {e}")
    finally:
        if driver:
            driver.quit()


def process_batch(company_list):
    """
    å¤šçº¿ç¨‹å¤„ç†å…¬å¸åˆ—è¡¨, æå‡é€Ÿåº¦
    å°† max_workers ä» 5 æ”¹ä¸º 10
    """
    write_log("ğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å–æ•°æ®... (max_workers=10)")
    with ThreadPoolExecutor(max_workers=5) as executor:
        executor.map(search_and_process, company_list)


def main():
    """æ‰‹åŠ¨è§¦å‘çš„çˆ¬è™«æµç¨‹"""
    companies = get_company_list_from_postgres()
    if not MINIO_CLIENT.bucket_exists(BUCKET_NAME):
        MINIO_CLIENT.make_bucket(BUCKET_NAME)

    write_log("ğŸ“¢ å¼€å§‹å¤„ç†å…¬å¸åˆ—è¡¨...")
    process_batch(companies)
    write_log("ğŸ‰ å…¨éƒ¨å…¬å¸å¤„ç†å®Œæˆï¼")


def schedule_scraper():
    """ä½¿ç”¨ APSchedulerï¼Œæ¯ 7 å¤©è¿è¡Œä¸€æ¬¡çˆ¬è™«"""
    scheduler = BlockingScheduler()
    scheduler.add_job(main, "interval", days=7)
    write_log("â³ Scraper scheduler started, running every 7 days...")

    try:
        scheduler.start()
    except (KeyboardInterrupt, SystemExit):
        write_log("ğŸ›‘ Scheduler stopped.")


if __name__ == "__main__":
    # é»˜è®¤è¿è¡Œä¸€æ¬¡çˆ¬è™«
    main()
